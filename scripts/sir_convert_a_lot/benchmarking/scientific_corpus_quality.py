"""Rubric and decision logic for Task 12 scientific-corpus benchmarking.

Purpose:
    Create/load deterministic manual rubric payloads and compute the
    quality-first backend decision with governance compatibility evaluation.

Relationships:
    - Consumes evaluation lane outputs from `scientific_corpus_execution.py`.
    - Produces `decision` and `governance_compatibility` payload sections.
"""

from __future__ import annotations

import json
import statistics
from pathlib import Path

from scripts.sir_convert_a_lot.domain.specs import JobStatus

from .scientific_corpus_types import (
    RUBRIC_WEIGHTS,
    CorpusFileInfo,
    DecisionSummary,
    GovernanceSummary,
    JobSpecProfile,
    LaneJobRecord,
    LaneResult,
    RankingEntry,
    RubricEntry,
    RubricPayload,
)
from .scientific_corpus_utils import utc_now_iso


def _clamp_score(value: int) -> int:
    return min(5, max(1, value))


def load_or_initialize_rubric(
    *,
    rubric_path: Path,
    corpus_files: list[CorpusFileInfo],
    backends: list[str],
) -> RubricPayload:
    """Load existing rubric or initialize deterministic placeholder entries."""
    expected_entries: dict[tuple[str, str], RubricEntry] = {}
    for file_info in corpus_files:
        for backend in backends:
            key = (file_info["document_slug"], backend)
            expected_entries[key] = {
                "source_file": file_info["source_file"],
                "document_slug": file_info["document_slug"],
                "backend": backend,
                "layout_fidelity": 3,
                "information_retention": 3,
                "legibility": 3,
                "notes": "autogenerated placeholder rubric; replace with manual review notes.",
            }

    auto_generated = not rubric_path.exists()
    provided_keys: set[tuple[str, str]] = set()
    if rubric_path.exists():
        payload_obj = json.loads(rubric_path.read_text(encoding="utf-8"))
        if not isinstance(payload_obj, dict):
            raise ValueError(f"Invalid rubric JSON object: {rubric_path}")
        entries_obj = payload_obj.get("entries")
        if not isinstance(entries_obj, list):
            raise ValueError(f"Rubric missing entries list: {rubric_path}")
        for item in entries_obj:
            if not isinstance(item, dict):
                auto_generated = True
                continue
            slug_obj = item.get("document_slug")
            backend_obj = item.get("backend")
            source_obj = item.get("source_file")
            layout_obj = item.get("layout_fidelity")
            retention_obj = item.get("information_retention")
            legibility_obj = item.get("legibility")
            notes_obj = item.get("notes")
            if not isinstance(slug_obj, str) or not isinstance(backend_obj, str):
                auto_generated = True
                continue
            if (
                isinstance(source_obj, str)
                and isinstance(layout_obj, int)
                and isinstance(retention_obj, int)
                and isinstance(legibility_obj, int)
                and isinstance(notes_obj, str)
            ):
                key = (slug_obj, backend_obj)
                expected_entries[key] = {
                    "source_file": source_obj,
                    "document_slug": slug_obj,
                    "backend": backend_obj,
                    "layout_fidelity": _clamp_score(layout_obj),
                    "information_retention": _clamp_score(retention_obj),
                    "legibility": _clamp_score(legibility_obj),
                    "notes": notes_obj,
                }
                provided_keys.add(key)
            else:
                auto_generated = True

    expected_keys = set(expected_entries.keys())
    if expected_keys.difference(provided_keys):
        auto_generated = True

    entries = [
        expected_entries[key]
        for key in sorted(expected_entries.keys(), key=lambda value: (value[1], value[0]))
    ]
    rubric_payload: RubricPayload = {
        "generated_at": utc_now_iso(),
        "auto_generated": auto_generated,
        "weights": dict(RUBRIC_WEIGHTS),
        "entries": entries,
    }
    rubric_path.parent.mkdir(parents=True, exist_ok=True)
    rubric_path.write_text(
        json.dumps(rubric_payload, indent=2, sort_keys=True) + "\n", encoding="utf-8"
    )
    return rubric_payload


def _weighted_score(entry: RubricEntry) -> float:
    weighted = (
        RUBRIC_WEIGHTS["layout_fidelity"] * entry["layout_fidelity"]
        + RUBRIC_WEIGHTS["information_retention"] * entry["information_retention"]
        + RUBRIC_WEIGHTS["legibility"] * entry["legibility"]
    )
    return round(weighted, 6)


def _build_profile_record_map(evaluation_lane: LaneResult) -> dict[str, dict[str, LaneJobRecord]]:
    profile_records: dict[str, dict[str, LaneJobRecord]] = {}
    for profile in evaluation_lane["profiles"]:
        mapping: dict[str, LaneJobRecord] = {}
        for record in profile["jobs"]:
            mapping[record["document_slug"]] = record
        profile_records[profile["profile_name"]] = mapping
    return profile_records


def compute_decision(
    *,
    evaluation_lane: LaneResult,
    rubric_payload: RubricPayload,
    corpus_files: list[CorpusFileInfo],
    production_profile: JobSpecProfile,
) -> tuple[DecisionSummary, GovernanceSummary]:
    """Compute quality-first backend decision and governance compatibility payloads."""
    rubric_lookup: dict[tuple[str, str], RubricEntry] = {}
    for entry in rubric_payload["entries"]:
        rubric_lookup[(entry["document_slug"], entry["backend"])] = entry

    profile_records = _build_profile_record_map(evaluation_lane)
    ranking: list[RankingEntry] = []
    for profile_name, records_by_slug in sorted(profile_records.items()):
        scores: list[float] = []
        severe_failures = 0
        for file_info in corpus_files:
            slug = file_info["document_slug"]
            record = records_by_slug.get(slug)
            rubric = rubric_lookup.get((slug, profile_name))
            if record is None or record["status"] != JobStatus.SUCCEEDED.value or rubric is None:
                scores.append(0.0)
                severe_failures += 1
                continue

            scores.append(_weighted_score(rubric))
            if (
                rubric["layout_fidelity"] <= 2
                or rubric["information_retention"] <= 2
                or rubric["legibility"] <= 2
            ):
                severe_failures += 1

        profile_summary = next(
            profile["summary"]
            for profile in evaluation_lane["profiles"]
            if profile["profile_name"] == profile_name
        )
        ranking.append(
            {
                "backend": profile_name,
                "median_weighted_score": round(statistics.median(scores), 6) if scores else 0.0,
                "severe_quality_failures": severe_failures,
                "success_rate": profile_summary["success_rate"],
                "latency_p50": profile_summary["latency_seconds"]["p50"],
            }
        )

    ranking.sort(
        key=lambda entry: (
            -entry["median_weighted_score"],
            entry["severe_quality_failures"],
            -entry["success_rate"],
            entry["latency_p50"],
            entry["backend"],
        )
    )

    quality_winner = ranking[0]["backend"] if ranking else "docling"
    winner_compatible = not (
        quality_winner == "pymupdf"
        and production_profile["acceleration_policy"] in {"gpu_required", "gpu_prefer"}
    )
    recommended_backend = quality_winner if winner_compatible else "docling"
    follow_up_required = not winner_compatible
    follow_up_note = (
        "Quality winner requires non-default governance profile; keep production recommendation "
        "governance-compatible and track follow-up decision task/ADR."
        if follow_up_required
        else None
    )

    decision: DecisionSummary = {
        "algorithm": "quality_first_v1",
        "ranking": ranking,
        "quality_winner": quality_winner,
        "recommended_production_backend": recommended_backend,
        "follow_up_required": follow_up_required,
        "follow_up_note": follow_up_note,
    }
    governance: GovernanceSummary = {
        "production_profile": production_profile,
        "quality_winner": quality_winner,
        "quality_winner_compatible_for_production": winner_compatible,
        "recommended_production_backend": recommended_backend,
        "notes": [follow_up_note]
        if follow_up_note is not None
        else ["Quality winner is governance-compatible."],
    }
    return decision, governance
