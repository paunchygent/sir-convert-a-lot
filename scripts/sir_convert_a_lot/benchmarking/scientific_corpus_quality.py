"""Rubric and manual-verdict logic for Task 12 scientific-corpus benchmarking.

Purpose:
    Create/load deterministic manual rubric payloads and produce
    manual-review-driven decision and governance compatibility sections.

Relationships:
    - Consumes evaluation lane outputs from `scientific_corpus_execution.py`.
    - Produces `decision` and `governance_compatibility` payload sections.
"""

from __future__ import annotations

import json
from pathlib import Path

from .scientific_corpus_types import (
    CorpusFileInfo,
    DecisionSummary,
    GovernanceSummary,
    JobSpecProfile,
    LaneResult,
    ManualVerdict,
    RubricEntry,
    RubricPayload,
)
from .scientific_corpus_utils import utc_now_iso


def _clamp_score(value: int) -> int:
    return min(5, max(1, value))


def load_or_initialize_rubric(
    *,
    rubric_path: Path,
    corpus_files: list[CorpusFileInfo],
    backends: list[str],
) -> RubricPayload:
    """Load existing rubric or initialize deterministic placeholder entries."""
    expected_entries: dict[tuple[str, str], RubricEntry] = {}
    for file_info in corpus_files:
        for backend in backends:
            key = (file_info["document_slug"], backend)
            expected_entries[key] = {
                "source_file": file_info["source_file"],
                "document_slug": file_info["document_slug"],
                "backend": backend,
                "layout_fidelity": 3,
                "information_retention": 3,
                "legibility": 3,
                "notes": "autogenerated placeholder rubric; replace with manual review notes.",
            }

    auto_generated = not rubric_path.exists()
    manual_review_completed = False
    manual_verdict: ManualVerdict | None = None
    provided_keys: set[tuple[str, str]] = set()
    if rubric_path.exists():
        payload_obj = json.loads(rubric_path.read_text(encoding="utf-8"))
        if not isinstance(payload_obj, dict):
            raise ValueError(f"Invalid rubric JSON object: {rubric_path}")
        entries_obj = payload_obj.get("entries")
        if not isinstance(entries_obj, list):
            raise ValueError(f"Rubric missing entries list: {rubric_path}")
        for item in entries_obj:
            if not isinstance(item, dict):
                auto_generated = True
                continue
            slug_obj = item.get("document_slug")
            backend_obj = item.get("backend")
            source_obj = item.get("source_file")
            layout_obj = item.get("layout_fidelity")
            retention_obj = item.get("information_retention")
            legibility_obj = item.get("legibility")
            notes_obj = item.get("notes")
            if not isinstance(slug_obj, str) or not isinstance(backend_obj, str):
                auto_generated = True
                continue
            if (
                isinstance(source_obj, str)
                and isinstance(layout_obj, int)
                and isinstance(retention_obj, int)
                and isinstance(legibility_obj, int)
                and isinstance(notes_obj, str)
            ):
                key = (slug_obj, backend_obj)
                expected_entries[key] = {
                    "source_file": source_obj,
                    "document_slug": slug_obj,
                    "backend": backend_obj,
                    "layout_fidelity": _clamp_score(layout_obj),
                    "information_retention": _clamp_score(retention_obj),
                    "legibility": _clamp_score(legibility_obj),
                    "notes": notes_obj,
                }
                provided_keys.add(key)
            else:
                auto_generated = True

        manual_review_obj = payload_obj.get("manual_review_completed")
        if isinstance(manual_review_obj, bool):
            manual_review_completed = manual_review_obj
        else:
            auto_generated = True

        verdict_obj = payload_obj.get("manual_verdict")
        if verdict_obj is None:
            manual_verdict = None
        elif isinstance(verdict_obj, dict):
            winner_obj = verdict_obj.get("quality_winner")
            recommended_obj = verdict_obj.get("recommended_production_backend")
            follow_up_required_obj = verdict_obj.get("follow_up_required")
            follow_up_note_obj = verdict_obj.get("follow_up_note")
            if (
                isinstance(winner_obj, str)
                and isinstance(recommended_obj, str)
                and isinstance(follow_up_required_obj, bool)
                and (follow_up_note_obj is None or isinstance(follow_up_note_obj, str))
            ):
                manual_verdict = {
                    "quality_winner": winner_obj,
                    "recommended_production_backend": recommended_obj,
                    "follow_up_required": follow_up_required_obj,
                    "follow_up_note": follow_up_note_obj,
                }
            else:
                auto_generated = True
        else:
            auto_generated = True

    expected_keys = set(expected_entries.keys())
    if expected_keys.difference(provided_keys):
        auto_generated = True

    entries = [
        expected_entries[key]
        for key in sorted(expected_entries.keys(), key=lambda value: (value[1], value[0]))
    ]
    rubric_payload: RubricPayload = {
        "generated_at": utc_now_iso(),
        "auto_generated": auto_generated,
        "manual_review_completed": manual_review_completed,
        "manual_verdict": manual_verdict,
        "entries": entries,
    }
    rubric_path.parent.mkdir(parents=True, exist_ok=True)
    rubric_path.write_text(
        json.dumps(rubric_payload, indent=2, sort_keys=True) + "\n", encoding="utf-8"
    )
    return rubric_payload


def compute_decision(
    *,
    evaluation_lane: LaneResult,
    rubric_payload: RubricPayload,
    corpus_files: list[CorpusFileInfo],
    production_profile: JobSpecProfile,
) -> tuple[DecisionSummary, GovernanceSummary]:
    """Compute manual-review-driven decision and governance payloads."""
    del evaluation_lane, corpus_files

    review_completed = rubric_payload["manual_review_completed"]
    verdict = rubric_payload["manual_verdict"]
    quality_winner: str | None = None
    recommended_backend: str | None = None
    follow_up_required = False
    follow_up_note: str | None = None

    if review_completed and verdict is not None:
        quality_winner = verdict["quality_winner"]
        recommended_backend = verdict["recommended_production_backend"]
        follow_up_required = verdict["follow_up_required"]
        follow_up_note = verdict["follow_up_note"]

    winner_compatible: bool | None = None
    governance_notes: list[str] = []

    if quality_winner is None or recommended_backend is None:
        governance_notes.append(
            "Manual quality verdict pending; no automatic winner/recommendation generated."
        )
    else:
        winner_compatible = not (
            quality_winner == "pymupdf"
            and production_profile["acceleration_policy"] in {"gpu_required", "gpu_prefer"}
        )
        if winner_compatible:
            governance_notes.append("Manual quality winner is governance-compatible.")
        else:
            recommended_backend = "docling"
            follow_up_required = True
            if follow_up_note is None:
                follow_up_note = (
                    "Manual quality winner conflicts with production governance profile; "
                    "keep production recommendation governance-compatible and track follow-up "
                    "decision task/ADR."
                )
            governance_notes.append(follow_up_note)

    decision: DecisionSummary = {
        "mode": "manual_review_only",
        "manual_review_completed": review_completed,
        "quality_winner": quality_winner,
        "recommended_production_backend": recommended_backend,
        "follow_up_required": follow_up_required,
        "follow_up_note": follow_up_note,
    }
    governance: GovernanceSummary = {
        "production_profile": production_profile,
        "quality_winner": quality_winner,
        "quality_winner_compatible_for_production": winner_compatible,
        "recommended_production_backend": recommended_backend,
        "notes": governance_notes,
    }
    return decision, governance
