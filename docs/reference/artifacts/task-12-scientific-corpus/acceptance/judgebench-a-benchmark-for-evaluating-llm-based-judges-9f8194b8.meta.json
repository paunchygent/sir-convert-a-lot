{
  "acceleration_used": "cuda",
  "backend_profile": "auto",
  "backend_used": "docling",
  "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
  "error_code": null,
  "job_id": "job_cf5961c266434d9885d390a825",
  "latency_seconds": 15.996915,
  "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.md",
  "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.meta.json",
  "retry_warnings_count": 0,
  "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf",
  "source_size_bytes": 1079243,
  "status": "succeeded",
  "warnings": []
}
