Published as a conference paper at ICLR 2025

## JUDGEBENCH: A BENCHMARK FOR EVALUATING LLM-BASED JUDGES

**Sijun Tan** [1] _[∗]_ **, Siyuan Zhuang** [1] _[∗]_ **, Kyle Montgomery** [2] _[∗]_ **, William Y. Tang** [1] **, Alejandro Cuadron** [1] **,**
**Chenguang Wang** [2] **, Raluca Ada Popa** [1] **, Ion Stoica** [1]

1UC Berkeley, 2Washington University in St. Louis
_{_ sijuntan,siyuan ~~z~~ huang _}_ @berkeley.edu
kylemontgomery@wustl.edu

ABSTRACT

LLM-based judges have emerged as a scalable alternative to human evaluation
and are increasingly used to assess, compare, and improve models. However, the
reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger
judges to evaluate them. Existing benchmarks primarily focus on a judge’s alignment with human preferences, but often fail to account for more challenging tasks
where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to
objectively evaluate LLM-based judges. Based on this framework, we propose
JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench
leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness.
Our comprehensive evaluation on a collection of prompted judges, fine-tuned
judges, multi-agent judges, and reward models shows that JudgeBench poses a
significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced
LLM-based judges. Data and code are available at [https://github.com/](https://github.com/ScalerLab/JudgeBench)
[ScalerLab/JudgeBench.](https://github.com/ScalerLab/JudgeBench)

1 INTRODUCTION

Large Language Models (LLMs) have achieved remarkable success in recent years and continue
to evolve at a rapid pace. With more advanced models emerging every month, a key challenge is
how to evaluate, compare, and supervise them effectively. While human judgment has traditionally
been the gold standard for evaluating language models, it is costly and time-consuming to collect
at scale. As a scalable alternative, LLM-based judges (Zheng et al., 2024) have gained widespread
adoption for ranking and evaluating models. Beyond evaluation, these judges also play a crucial role
in improving models, serving as reward models during training (Yuan et al., 2024; Luo et al., 2024a)
and acting as verifiers during inference to select the best response from multiple candidates (Cobbe
et al., 2021; Lightman et al., 2023).

Despite the widespread adoption, a fundamental question remains: How reliable are these LLMbased judges themselves? Since LLMs themselves are prone to make logical and factual mistakes,
how can we trust that LLM-based judges are accurate and objective? To evaluate LLM-based judges,
many prior works have focused on these judges’ agreement with human preference (Dubois et al.,
2024; Zheng et al., 2024; Zhang et al., 2023; Wang et al., 2023a). The core assumption implied in
these works is that crowdsourced human annotators will evaluate the responses objectively and not
make mistakes. This assumption may hold when the problem is straightforward but falters when the
tasks grow more complex. For more complex evaluations that require thoughtful reasoning, such

\*Equal contribution

1

Published as a conference paper at ICLR 2025

Figure 1: Comparison of JudgeBench against previous works. Unlike previous works which focus
on instruction following or stylistic preferences, the focus of JudgeBench is on evaluating the factual
and logical correctness of complex responses to challenging questions. JudgeBench is noticeably
more difficult than previous work, containing responses that are challenging for crowdsourced human annotators to evaluate in a reliable and timely manner.

as verifying the correctness of code snippets or evaluating intricate mathematical proofs, humans
are prone to make mistakes. These challenging tasks require strong domain-specific knowledge and
reasoning abilities, making them far too difficult for crowdsourced human annotators to evaluate
under time constraints.

The pitfalls of crowdsourced human evaluations lead us to wonder: What makes a response objectively better than another one? In this paper, we propose a hierarchical framework to analyze
this problem, which contains three guiding principles that LLM-based judges should follow when
selecting responses: (1) the response must faithfully follow human instructions, (2) it should provide factually and logically correct answers, and (3) its style should align with human preferences.
Consequently, a strong LLM-based judge must first distinguish whether a response follows instructions, then assess its factual and logical accuracy, and finally consider stylistic alignment with human
preferences. For example, suppose the question is “What is the capital of Spain?”. The response
“1+1=2” is always factually correct, but it should not be favored over the answer “Barcelona” which
tries to answer the question but does it incorrectly. Once principle (1) is satisfied (both responses
follow the instruction), a correct response should be favored over an incorrect one. Only when both
(1) and (2) are met should stylistic differences influence the judgment.

While instruction following and style are relatively easy for human annotators to judge, factual
and logical correctness becomes increasingly challenging with complex problems. In such cases,
human evaluators may mistakenly favor responses that seem more plausible or are simply longer,
prioritizing style over correctness—thereby violating the hierarchical framework. As a result, human
evaluations often become unreliable as the difficulty of the task increases.

To objectively evaluate LLM-based judges, it is crucial to adhere strictly to this hierarchy, distinguishing objective metrics such as factual correctness and instruction following from subjective factors like stylistic preferences. The LLMBar benchmark (Zeng et al., 2023) follows a similar intuition
by assessing instruction following, but no existing work has systematically focused on evaluating
factual and logical correctness as question complexity scales with increasingly advanced LLMs. As
AI models surpass human capabilities, their responses become harder for both human and LLMbased judges to assess. Ensuring AI judges evolve alongside these models is essential for accurately
evaluating complex responses. Thus, there is an pressing need for a rigorous, objective methodology
to assess LLM judges based on their reasoning abilities.

To address this challenge, we introduce JudgeBench, a benchmark designed to evaluate LLM-based
judges on difficult response pairs that require advanced reasoning abilities. Our main insight is that
_if a model struggles to consistently generate correct, coherent responses to a challenging question,_
_it_ _will_ _also_ _struggle_ _to_ _distinguish_ _between_ _its_ _correct_ _and_ _incorrect_ _responses._ Leveraging this

2

Published as a conference paper at ICLR 2025

insight, we build a novel pipeline that transforms any dataset with ground truth labels and verification algorithms into a corresponding dataset specifically tailored for LLM-based judges. Using this
pipeline, we construct a challenging dataset consisting of 350 response pairs across four categories:
general knowledge, reasoning, mathematics, and coding. Each pair contains one objectively correct
response and one objectively incorrect response, with the incorrect response designed to contain
subtle errors, making it difficult for LLM-based judges to distinguish between the two. Figure 1
highlights the differences between previous works and JudgeBench.

The key contributions of this paper are as follows:

```
- We propose a principled evaluation framework for LLM-based judges, prioritizing factual and logical correctness over stylistic alignment, offering guidance for designing future
```

evaluation datasets in this domain.

```
- Based on this framework, we develop a novel pipeline that can transform any dataset with
```

objective ground truth labels into a corresponding dataset tailored for LLM-based judges.

```
- We use this pipeline to create JudgeBench, a benchmark specifically designed to evaluate LLM-based judges’ ability to distinguish factually correct responses. Comprehensive
```

evaluation shows that JudgeBench poses a significantly greater challenge than prior benchmarks, providing a robust test bed for future research on reasoning-enhanced judges.

2 RELATED WORK

**LLM-based judges.** The use of large language models (LLMs) as judges has become an increasingly popular approach for evaluating AI-generated outputs. These approaches can be broadly categorized into three types: prompted judges, fine-tuned judges, and multi-agent judges. Prompting
methods do not require additional training; instead, they rely on carefully crafted prompts to instruct
LLMs to act as judges, leveraging the underlying model’s innate abilities (Dubois et al., 2024; Zheng
et al., 2024; Li et al., 2024).

Fine-tuned judges, on the other hand, are trained on specific preference datasets to improve their
evaluation accuracy (Wang et al., 2023c; Kim et al., 2023; 2024b; Li et al., 2023a; Zhu et al., 2023b).
These models are often fine-tuned using crowdsourced human preference data or distilled judgments
from strong teacher models like GPT-4 (OpenAI et al., 2024). While fine-tuned judges tend to
perform well on benchmarks, Huang et al. (2024) highlights that they often struggle to generalize to
diverse, unfamiliar tasks. Additionally, because the preference datasets used for fine-tuning typically
do not contain sufficiently challenging examples, they fail to enhance the reasoning abilities of the
judges, limiting their overall effectiveness.

Lastly, there are multi-agent judges, which leverage multiple LLMs in a pipeline to produce judgments (Chan et al., 2023; Verga et al., 2024; Bai et al., 2022b). By combining the outputs of several
LLMs, these systems can surpass the capabilities of a single model, offering more robust evaluations. However, this approach comes with the trade-off of significantly higher computational costs
during inference.

**Reward** **models** **and** **verifiers.** Reward models (RMs) are closely related to, but distinct from,
LLM-based judges. RMs are primarily used in reinforcement learning from human feedback
(RLHF) (Christiano et al., 2017; Ziegler et al., 2019) to align pre-trained LLMs with human preferences. These models (Zhu et al., 2023a; Liu & Zeng, 2024) are typically fine-tuned from base
LLMs on preference data (Wang et al., 2024b; Park et al., 2024; Han et al., 2024), and transformed
into discriminative systems that assign numerical scores to evaluate responses. RMs learn to convert
preference signals into quantitative judgments, steering models toward more preferred behaviors.

Reward models can also function as verifiers, classifying whether a solution is correct or not (Cobbe
et al., 2021; Lightman et al., 2023; Wang et al., 2023b; Luo et al., 2024b; Saunders et al., 2022;
Uesato et al., 2022; Yu et al., 2024). As verifiers, they can select the best-of-N responses from
an LLM, improving overall response quality. While most reward models are discriminative, recent
research has explored the use of generative models (LLMs) as verifiers (Zhang et al., 2024), leveraging LLMs’ generative abilities to enhance reasoning capabilities. Although LLM-based judges
are distinct from reward models, they can be viewed as a form of generative reward model, as their

3

Published as a conference paper at ICLR 2025

Figure 2: Overview of JudgeBench Pipeline. Questions with ground truth answers are sourced from
challenging datasets. We sample _k_ responses to each question using a strong LLM (e.g., GPT-4o)
and grade each response for correctness. Response pairs are constructed from correct and incorrect
responses. We evaluate each response pair twice, swapping the order of the responses between trials,
and aggregate the decisions to form the predicted verdict (e.g., _B_ _> A_ ).

preferences can also be used in RLHF to align LLMs. This suggests that these two fields are closely
related and are gradually converging.

**Benchmarks for LLM-based judges and reward models.** As LLM-based judges have become a
widely adopted method for evaluating and improving large language models (LLMs), several benchmarks have been introduced to assess their effectiveness. Works such as LLMEval (Zhang et al.,
2023), MTBench (Zheng et al., 2024), and FairEval (Wang et al., 2023a) focus on evaluating the
alignment between LLM-based judges’ responses and human evaluations. As mentioned above,
these dataset suffers from the inherent subjectivity of human evaluation, prioritizing stylistic differences over factual and logical correctness. LLMBar (Zeng et al., 2023) instead takes a different
approach by assessing LLM-based judges’ ability to follow instructions, using response pairs with
clear ground truth preference labels based on adherence to instructions rather than subjective preferences. In contrast, JudgeBench focuses on assessing LLM-based judges’ ability to reason through
responses and distinguish between correct and incorrect responses, which is more challenging than
instruction following alone.

On the reward model side, RewardBench (Lambert et al., 2024) is a benchmark that offers a comprehensive evaluation of reward models’ ability in domains such as safety, chat, and reasoning. The
aggregation over several prior preference datasets and benchmarks (Li et al., 2023b; Zheng et al.,
2024; Zeng et al., 2023; Lightman et al., 2023; Muennighoff et al., 2023; R¨ottger et al., 2023; Wang
et al., 2023d; Bai et al., 2022a; Askell et al., 2021; Ethayarajh et al., 2022; Stiennon et al., 2020).
Compared to RewardBench’s reasoning datasets, JudgeBench proves to be significantly more challenging, as demonstrated by our experiments in Section 4.3.

3 JUDGEBENCH

**JudgeBench’s** **pipeline.** How can we generate challenging response pairs that are difficult for
LLM-based judges to distinguish while maintaining objective ground truth labels? Revisiting principle (2), which underpins our work, we assert that when both responses follow human instructions
faithfully, the factually and logically correct response should be favored. Our main idea to achieve
this objective is to leverage an existing challenging dataset with ground truth labels and develop a
pipeline to transform it into a set of response pairs. Specifically, if a dataset includes an algorithm
to verify correctness, we can identify response pairs where one response passes verification and the
other does not. The incorrect response may either fail to follow instructions or contain factual errors,
ensuring clear objective ground truth labels aligned with our evaluation principle.

4

Published as a conference paper at ICLR 2025

A straightforward method to generate response pairs is to use multiple LLMs to produce candidate responses and then select one correct and one incorrect response per pair. While this ensures
objective ground truth labels, it introduces several limitations. First, since LLM capabilities vary,
incorrect responses may be too easily identifiable, reducing the dataset’s difficulty and undermining
the challenge for LLM-based judges. Second, because models have distinct stylistic tendencies,
judges may rely on superficial differences rather than factual correctness, conflicting with our goal
of evaluating reasoning ability. Lastly, LLM-based judges exhibit self-enhancement bias (Zheng
et al., 2024), often favoring responses generated by the same model, making it difficult to measure
and mitigate this bias when multiple models are involved.

To address these issues, we take an alternative approach based on our key insight: _if a model strug-_
_gles to consistently generate correct, coherent responses to a challenging question, it will also strug-_
_gle_ _to_ _differentiate_ _between_ _those_ _responses._ Our proposed pipeline (Figure 2) refines the initial
approach to mitigate these pitfalls. Given a set of questions from an existing dataset, we first sample
_k_ responses from a strong model (e.g., GPT-4o) and evaluate their correctness. We then filter out
questions where all _k_ responses are either correct or incorrect, retaining only those with at least one
correct and one incorrect response to construct response pairs with objective ground truth labels.

As a result, the generated dataset is inherently more challenging for LLM-based judges. Since
all candidate responses are produced by a single model, this method also ensures consistency in
response style, reducing the influence of stylistic differences and mitigating self-enhancement bias
in the judgments. However, this approach introduces a different kind of bias. Because the base model
generates the responses, the dataset may be disproportionately challenging for that particular model
compared to others, as different models may not struggle with the same questions. Nevertheless,
this bias is confined to the model used for response generation, while creating a level playing field
for all other models. In Section 4.4, we conduct an ablation study to examine the extent of this bias.

**JudgeBench’s** **datasets.** JudgeBench’s pipeline is flexible and dynamic, capable of transforming any existing dataset with ground truth labels and verification mechanisms into a response pair
format for evaluating LLM-based judges. To ensure that the resulting response pairs are challenging to distinguish, the source dataset itself must present a significant level of difficulty. To assess
JudgeBench’s ability to effectively test LLM-based judges, we categorize our datasets into four distinct categories: **Knowledge**, **Reasoning**, **Mathematics**, and **Coding** . We select datasets that align
with these categories and meet the challenge criteria.

```
- **MMLU-Pro** (Wang et al., 2024a). We use MMLU-Pro for the **Knowledge** category. MMLU-Pro is a challenging multi-task dataset, filtered from the original MMLU
```

dataset (Hendrycks et al., 2020). It includes 12,032 college-level exam questions across 14
disciplines (e.g., Physics, Chemistry, Law), each presented as a multiple-choice question
with up to 10 possible options.

```
- **LiveBench** (White et al., 2024). LiveBench offers datasets in categories such as reasoning,
```

mathematics, and instruction-following, and releases new data monthly to avoid contamination. For the **Reasoning** and **Mathematics** categories, we use the corresponding LiveBench
datasets. The reasoning problems come from sources such as Big-Bench Hard (Suzgun
et al., 2022), and Zebra Puzzles, while the math problems are drawn from math competitions (e.g., AMC12, USAMO).

```
- **LiveCodeBench** (Jain et al., 2024). LiveCodeBench is a contamination-free dataset for
```

coding tasks, containing over 300 challenging questions sourced from coding contests like
LeetCode, AtCoder, and Codeforces. We select this dataset for the **Coding** category.

**Data Filtering and selection.** Each of the datasets mentioned above provides a ground truth answer and an algorithm to evaluate the correctness of model outputs. For instance, MMLU-Pro verifies solutions based on regex string matching. During our pipeline execution, we found that some
responses were marked incorrect due to minor formatting issues, even though their solutions were
correct. Constructing pairs with these responses is problematic, as the “incorrect” response may
simply fail the automated check due to a slight format mismatch. This gives the judge an unintended
shortcut, reducing the quality of the dataset.

To address this, we used an additional LLM (GPT-4o-mini) to verify the correctness of solutions.
The model was prompted to extract the solution from the response and determine whether it was

5

Published as a conference paper at ICLR 2025

correct, regardless of format. We filtered out responses where the LLM and the automated solution checker disagreed. Upon manual inspection of these disagreements, we confirmed they were
indeed caused by format errors. An example case where the two methods disagree can be found in
Appendix A.5. We perform some additional randomized filtering on MMLU-Pro and LiveBench to
better balance the size of each subset (see Appendix A.3 for details).

After applying our pipeline with GPT-4o as the underlying model and incorporating the additional
filtering, our dataset consists of a total of 350 questions: 154 in **Knowledge**, 98 in **Reasoning**, 56 in
**Mathematics**, and 42 in **Coding** .

4 EVALUATION

LLM-based judges are known to exhibit positional bias (Zheng et al., 2024; Wang et al., 2023a),
where the order in which the response pairs are presented can influence their decision. Evaluating
the judge on a single order of responses introduces this bias into evaluation. To mitigate this, we
evaluate the LLM-based judge twice, swapping the order of the response pairs in the second trial.

Since our response pairs contain an objectively correct and incorrect response, the only valid decisions are _A_ _>_ _B_ and _A_ _\<_ _B_ . However, in practice, some judges support a tie option: _A_ = _B_ .
To address this discrepancy, we aggregate the results from both trials as follows: if both trials yield
_A_ _>_ _B_ or one trial gives _A_ _>_ _B_ and the other _A_ = _B_, we consider the aggregate decision to be
_A_ _>_ _B_ . Inconsistent decisions (e.g., _A_ _>_ _B_ in one trial, _A_ _\<_ _B_ in the other) or ties in both trials
are deemed incorrect, as they indicate the judge is either guessing or unable to reliably distinguish
between responses. This method enables a more accurate measurement of the judges’ ability.

4.1 EVALUATING LLM-BASED JUDGES ON JUDGEBENCH.

In this subsection, we describe three major experiments we conduct on JudgeBench. First, we
assess LLM-based judges from prior literature, which can be categorized as prompted, fine-tuned,
and multi-agent judges. Second, we use JudgeBench as a proxy to evaluate the underlying LLM’s
performance by fixing the prompt and varying the models. Lastly, we apply JudgeBench to evaluate
reward models. In Section 4.2, we provide a detailed analysis of these results.

**Evaluating** **LLM-based** **judges** **across** **categories.** We evaluate the following three categories
of LLM-based judges on JudgeBench. Additional details about these judges can be found in Appendix A.1.

```
- **Prompted Judges.** For prompted judges, we include the **Vanilla** judge, adapted from AlpacaFarm (Dubois et al., 2024), which directly prompts the LLM to indicate its preferred
```

response without requiring an explanation. We also consider the **Arena-Hard** **Judge** (Li
et al., 2024), which prompts the LLM to first generate its own reference answer, and then
analyze both responses before delivering a final verdict. We also include Google’s **Ver-**
**texAI Evaluation** service (Cloud, 2024) in this category.

```
- **Fine-tuned** **Judges.** For fine-tuned judges, we evaluate **PandaLM** (Wang et al., 2023c)
```

(fine-tuned on LLaMA-7B (Touvron et al., 2023a)), **Prometheus2** (Kim et al., 2024b) (finetuned on Mistral-7B/Mixtral-8x7B), **JudgeLM** (Zhu et al., 2023b) (fine-tuned on Vicuna7B/13B/33B (Chiang et al., 2023)), **AutoJ** (Li et al., 2023a) (fine-tuned on LLaMA-2-13Bchat (Touvron et al., 2023b)), and **Skywork** ’s judges (Shiwen et al., 2024) fine-tuned on
Llama-3.1-8B/70B (Dubey et al., 2024). These models are fine-tuned using either crowdsourced preference datasets or on distilled GPT-4 judgments.

```
- **Multi-Agents Judges.** For multi-agent judges, we evaluate **ChatEval** (Chan et al., 2023),
```

which leverages multiple LLMs in a debate to produce the final judgment.

**Evaluating** **JudgeBench** **on** **different** **models.** JudgeBench can also be used as a benchmark to
evaluate the underlying model’s capability. To evaluate the ability of these models, we freeze the
Arena-Hard Judge’s prompt and change the underlying model to see how the performance of different models varies. We select the latest models from five model providers: OpenAI, Anthropic,

6

Published as a conference paper at ICLR 2025

Knowledge Reasoning Math Coding Overall

**Prompted Judges**

Vanilla (GPT-4o) 44.16 47.96 66.07 **61.90** 50.86
Arena-Hard Judge (GPT-4o) 50.65 54.08 **75.00** 59.52 56.57
VertexAI Evaluation (Gemini-1.5-pro) 45.45 44.90 53.57 28.57 44.57

**Fine-tuned Judges**

PandaLM 9.09 21.43 7.14 16.67 13.14
Prometheus2-7b 38.31 25.51 35.71 42.86 34.86
Prometheus2-8x7b 41.56 39.80 50.00 23.81 40.29
Prometheus2-bgb-8x7b 45.45 30.61 46.43 28.57 39.43
JudgeLM-7B 23.38 29.59 32.14 11.90 25.14
JudgeLM-13B 26.62 29.59 28.57 19.05 26.86
JudgeLM-33B 32.47 48.98 33.93 19.05 35.71
AutoJ 40.26 29.59 44.64 28.57 36.57
Skywork-LLaMA-3.1B-8B 51.30 54.08 73.21 33.33 53.43
Skywork-LLaMA-3.1B-70B **55.84** **55.10** 73.21 47.62 **57.43**

**Multi-Agent Judges**

ChatEval 32.47 31.63 44.64 30.95 34.00

Table 1: Evaluating LLM-based judges on JudgeBench.

Meta, Google, and DeepSeek. Some of these models we host ourselves and for others, we rely on
either official or third-party APIs. We contain the details of our model sources in Appendix A.1.

Model Knowledge Reasoning Math Coding Overall

GPT-4o 50.65 54.08 75.00 59.52 56.57
GPT-4o-mini 48.05 43.88 69.64 45.24 50.00
o1-preview 66.23 79.59 85.71 85.71 75.43
o1-mini 58.44 62.24 82.14 78.57 65.71
o3-mini (high) **67.53** **89.80** **87.50** **100.0** **80.86**
o3-mini (medium) 62.34 86.73 85.71 92.86 76.57
o3-mini (low) 62.99 69.39 83.93 83.33 70.57

Claude-3.5-Sonnet 62.34 66.33 66.07 64.29 64.29
Claude-3-Haiku 35.06 34.69 33.93 21.43 33.14

Llama-3.1-405B-Instruct 55.84 54.08 69.64 50.00 56.86
Llama-3.1-70B-Instruct 51.30 48.98 60.71 52.38 52.29
Llama-3.1-8B-Instruct 38.31 45.92 44.64 33.33 40.86

Gemini-1.5-pro 49.35 42.86 64.29 26.19 47.14
Gemini-1.5-flash 42.86 36.73 50.00 21.43 39.71

Deepseek-R1 59.09 82.65 80.36 92.86 73.14

Table 2: Evaluating the Arena-Hard Judge on JudgeBench, with different underlying models.

**Evaluating** **JudgeBench** **on** **reward** **models.** While our primary focus is on evaluating LLMbased judges, JudgeBench can also be used to assess reward models, which are trained on preference
data to evaluate model outputs. Unlike pairwise LLM-based judges, reward models independently
assign scores to each response, and the higher-scoring response is deemed the preferred one.

In our experiments, we evaluated several top-performing reward models from the RewardBench
leaderboard (Lambert et al., 2024), including Skywork Reward’s model (Liu & Zeng, 2024), InternLM’s reward models (Cai et al., 2024), and the GRM-Gemma-2B (Yang et al., 2024) reward
model fine-tuned on Google’s Gemma model (Team et al., 2024). Results are presented in Table 3

7

Published as a conference paper at ICLR 2025

Reward Model Knowledge Reasoning Math Coding Overall

Skywork-Reward-Gemma-2-27B 59.74 66.33 83.93 50.00 64.29
Skywork-Reward-Llama-3.1-8B 59.09 64.29 76.79 50.00 62.29
InternLM2-20B-Reward 62.34 69.39 66.07 50.00 63.43
InternLM2-7B-Reward 56.49 61.22 71.43 50.00 59.43
GRM-Gemma-2B 62.99 53.06 64.29 54.76 59.43

Table 3: Evaluating reward models on JudgeBench.

4.2 ANALYSIS AND TAKEAWAYS OF EVALUATION RESULTS ON JUDGEBENCH.

**LLM-based judges’ performance falls short under JudgeBench’s challenging questions.** The
evaluation results from Table 1 and 2 highlight the difficulty of JudgeBench. Even a strong model
like GPT-4o struggles, achieving accuracy no better than random guessing when using the vanilla
prompt. Although the more advanced Arena-Hard prompt improves performance slightly (from 50%
to 56%), the overall accuracy remains low.

All of our fine-tuned judges (except Skywork) perform significantly below the random baseline.
We explore several reasons behind the relatively poor performance of fine-tuned judges in Appendix A.2. Among all the fine-tuned judges, Skywork’s LLM-based judges (Shiwen et al., 2024)
perform the best overall, with an overall accuracy of 57 _._ 43%. When compared to the base Llama-3.1
models in Table 2, fine-tuning shows a clear performance boost, improving accuracy by over 12%
for the 8B model and 5% for the 70B model.

From Table 2, we can see that there is a clear gap in the performance between different models,
with larger models generally performing better than their smaller counterparts across all providers.
Among all models, OpenAI’s o3-mini performs the best overall, achieving 80 _._ 86%, 76 _._ 57% and
70 _._ 57% accuracy at high, medium, and low reasoning levels respectively. These reasoningenhanced models differ from standard models by taking extra time to “think” before generating
a response (OpenAI, 2024). The superior results indicate that scaling test-time compute is a promising path to improve the reasoning ability of the judges. Beyond these models, Claude-3.5-Sonnet
ranks highest among general-purpose models with an accuracy of 64 _._ 29%. Despite these results
being well above the random baseline, all models still have considerable room for improvement.

**Reward** **models’** **performance** **is** **on** **par** **with** **much** **more** **powerful** **LLMs.** When comparing
the performance of reward models to LLM-based judges (Table 1), we find that fine-tuned reward
models generally outperform LLM-based judges. For example, Skyworks’s Gemma-2-27B reward
model achieves accuracy comparable to Claude-3.5-Sonnet, one of the most advanced LLMs currently available, and Skywork’s Llama-3.1-8B reward model surpasses the performance of the base
model by a huge margin (62.29% vs 40.86%). The above observation indicates that training a specialized verifier from a weak model to judge a stronger model is possible.

Our results also show that reward models exhibit a smaller performance gap on JudgeBench, with
overall accuracies ranging from approximately 59% to 64%. This consistency is likely due to these
models being trained or fine-tuned on similar open preference datasets, such as HelpSteer2 (Wang
et al., 2024b) and Skywork-Preference-80 (Skywork, 2023). While model size does influence performance, as seen in the higher accuracy of InternLM-20B compared to InternLM-7B and Skywork’s
Gemma-2-27B outperforming LLaMA-3.1-8B, the improvements are modest. This suggests that the
quality of the training datasets plays a more critical role in shaping the preferences of reward models
than model size alone.

**Advancing** **the** **reasoning** **ability** **of** **LLM-based** **judges** **is** **the** **next** **frontier.** Our evaluation
on JudgeBench highlights the limitations of current LLM-based judges in distinguishing between
challenging response pairs. As AI systems become more sophisticated, LLM-based judges risk becoming a bottleneck to further scaling. For example, Brown et al. (2024) demonstrated that repeated
sampling can improve a model’s coverage (the percentage of problems solved) when an oracle-level
verifier is available. However, when the verifier is not strong enough, it becomes the limiting factor
in this process. Thus, enhancing the reasoning capabilities of LLM-based judges is essential for ad

8

Published as a conference paper at ICLR 2025

100

90

80

70

60

50

40

30

Figure 3: Comparison of JudgeBench against prior benchmarks for LLM-based judges.

vancing the overall performance of AI systems–an area that remains largely underexplored. While
we leave improving the LLM-based judges as future work, JudgeBench provides a robust test bed
for evaluating future reasoning-enhanced judges.

4.3 COMPARING JUDGEBENCH TO OTHER EXISTING BENCHMARKS.

**Comparison** **with** **MTBench,** **LLMEval,** **FairEval,** **and** **LLMBar** To compare with existing
benchmarks for LLM-based judges, we evaluate five models (GPT-4o, Claude-3.5-Sonnet, Llama3.1-70B-Instruct, Llama-3.1-8B-Instruct, and Claude-3.5-Haiku) on existing datasets using the
Arena-Hard prompt. For fairness, we apply the same evaluation procedure described in Section 4,
where the judge is run twice on each pair, and its final decision is based on the aggregated judgments.

Our results show that JudgeBench is the most challenging dataset for evaluating LLM-based judges.
The strongest model on JudgeBench achieves only 64% accuracy, the lowest among all five datasets.
Additionally, JudgeBench demonstrates strong separability, with a 31% performance gap between
the best-performing model (Claude-3.5-Sonnet) and the weakest (Claude-3.5-Haiku). This gap is
comparable to LLMBar: Adversarial, which has a 33% gap, indicating that JudgeBench is a strong
benchmark for evaluating LLM-based judges.

**Comparison** **with** **RewardBench.** RewardBench (Lambert et al., 2024) is a general benchmark
for evaluating reward models, with a subcategory dedicated to reasoning in reward models. It
includes PRM Math (Lightman et al., 2023) and HumanEvalPack (Muennighoff et al., 2023) as
benchmarks for reasoning tasks. These datasets are similar to the Math and Coding categories in
JudgeBench, with PRM Math evaluating correct versus incorrect math proofs and HumanEvalPack
comparing correct versus buggy code. However, these benchmarks are very saturated, with the
strongest model achieving up to 97% accuracy on these datasets. This saturation is likely due to data
contamination since datasets such as PRM-800k are widely used in training reward models nowadays. In contrast, JudgeBench is far more challenging, with top reward models achieving only 64%
accuracy. Thus, JudgeBench offers a valuable complement to RewardBench for evaluating reward
models on difficult tasks requiring reasoning.

4.4 ABLATION STUDIES

**Is verifying a problem’s solution easier than solving the problem itself?** Intuitively, verification
should be simpler, as the model is provided with candidate solutions and only needs to identify the
correct one, a task that would yield 50% accuracy through random guessing alone. To explore this,
we conducted an ablation study in which we prompted models to directly solve the problem and
compared their accuracy to that of the judge. Our results, presented in Table 4, show that for a fixed
model, the judge’s accuracy closely mirrors that of the solver. While GPT-4o’s and Gemini-1.5-Pro’s
judges slightly outperform their corresponding solvers, Claude-3.5-Sonnet’s and Llama-3.1-405BInstruct’s judges lag behind their respective solvers.

Although the overall accuracy between the solver and judge is close, we observe a notable discrepancy in the Coding category, where the solver consistently outperforms the judge across all models.
Conversely, in the Math category, judges significantly outperform solvers. This suggests that coding
problems are more difficult to evaluate, while logical errors in math problems are generally easier

9

Published as a conference paper at ICLR 2025

Setup Knowledge Reasoning Math Coding Overall

GPT-4o Solver 48.70 53.06 58.93 73.81 54.57
GPT-4o Judge 50.65 54.08 75.00 59.52 56.57

Claude-3.5-Sonnet Solver 61.04 62.24 60.71 88.10 64.57
Claude-3.5-Sonnet Judge 62.34 66.33 66.07 64.29 64.29

Llama-3.1-405B-Instruct Solver 48.05 67.86 63.27 66.67 57.71
Llama-3.1-405B-Instruct Judge 55.84 54.08 69.64 50.00 56.86

Gemini-1.5-pro Solver 33.12 42.86 37.50 64.29 40.29
Gemini-1.5-pro Judge 49.35 42.86 64.29 26.19 47.14

Table 4: Evaluating the LLM’s ability to solve the problems.

to identify. Overall, this ablation study indicates that the ability of the judge to verify the solution
pairs is highly correlated with its ability to solve the problem itself.

**Investigating** **bias** **of** **the** **pipeline.** In JudgeBench’s pipeline, we use GPT-4o to generate all response pairs. This introduces a bias against GPT-4o judges, as the generated pairs are inherently
challenging for GPT-4o itself to distinguish. To empirically investigate this bias, we conduct an
ablation study using Claude-3.5-Sonnet instead to generate response pairs. This results in 270 pairs,
154 pairs for Knowledge, 51 pairs for Reasoning, 34 pairs for Math, and 31 pairs for Coding.

ated pairs.

Figure 4 presents a side-by-side comparison of several models’ performance on GPT-4o pairs versus
Claude-3.5-Sonnet pairs. The results confirm our hypothesis: Claude-3.5-Sonnet, which achieves
64 _._ 3% accuracy on GPT-4o pairs, drops to 44 _._ 8% accuracy when tasked with judging its own generated pairs. Similarly, GPT-4o tops the Claude-3.5-Sonnet pairs with 53 _._ 9% accuracy. However, this
number is still slightly lower than the 56 _._ 6% accuracy on its own pairs. This suggests that Claude3.5-Sonnet is a stronger reasoning model, producing more difficult-to-distinguish response pairs in
general. Beyond the models used to generate the response pairs, other models exhibit similar performance gaps, indicating that the response pairs remain consistent in evaluating model capabilities,
regardless of the model used to generate the pairs.

CONCLUSION

In this work, we introduce a novel hierarchical evaluation framework to objectively evaluate LLMbased judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLMbased judges’ ability to distinguish factually and logically correct outputs. Our work addresses the
pressing need to evaluate LLM-based judges’ reasoning ability, which is of increasing importance
given the rapid advancement of AI intelligence today. We hope that our framework and benchmark
can offer insights into future dataset design and foster further research into this space.

10

Published as a conference paper at ICLR 2025

REFERENCES

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,
Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory
for alignment. _arXiv preprint arXiv:2112.00861_, 2021.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. _arXiv_ _preprint_ _arXiv:2204.05862_,
2022a.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022b.

Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher R´e, and
Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling.
_arXiv preprint arXiv:2407.21787_, 2024.

Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui
Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye
Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting
Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li,
Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun
Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang
Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song,
Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang,
Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong,
Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia
Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo
Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui
Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou,
Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report, 2024.

Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and
Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. _arXiv_
_preprint arXiv:2308.07201_, 2023.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%\* chatgpt quality, March 2023. URL [https:](https://lmsys.org/blog/2023-03-30-vicuna/)
[//lmsys.org/blog/2023-03-30-vicuna/.](https://lmsys.org/blog/2023-03-30-vicuna/)

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. _Advances in neural information processing sys-_
_tems_, 30, 2017.

Google Cloud. Generative ai evaluation service overview, 2024. URL [https:](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)
[//cloud.google.com/vertex-ai/generative-ai/docs/models/](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)
[evaluation-overview.](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
_arXiv preprint arXiv:2407.21783_, 2024.

Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. _Advances in Neural Information Processing Systems_,
36, 2024.

11

Published as a conference paper at ICLR 2025

Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with _V_ usable information. In _International Conference on Machine Learning_, pp. 5988–6008. PMLR,
2022\.

Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin
Choi, and Nouha Dziri. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks,
and refusals of llms, 2024. [URL https://arxiv.org/abs/2406.18495.](https://arxiv.org/abs/2406.18495)

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv_ _preprint_
_arXiv:2009.03300_, 2020.

Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Tianfu Wang, Zhengyu Chen, Nicholas Jing
Yuan, Jianxun Lian, Kaize Ding, and Hui Xiong. Explaining length bias in llm-based preference
evaluations, 2024. [URL https://arxiv.org/abs/2407.01085.](https://arxiv.org/abs/2407.01085)

Hui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. On the
limitations of fine-tuned judge models for llm evaluation. _arXiv preprint arXiv:2403.02839_, 2024.

Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando
Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free
evaluation of large language models for code. _arXiv preprint arXiv:2403.07974_, 2024.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.

Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le
Scao, Th´eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed.
Mixtral of experts. 2024. [URL https://arxiv.org/abs/2401.04088.](https://arxiv.org/abs/2401.04088)

Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In _The Twelfth International Conference on Learning Repre-_
_sentations_, 2023.

Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin
Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo,
Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo,
Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen
Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. The biggen
bench: A principled benchmark for fine-grained evaluation of language models with language
models, 2024a. [URL https://arxiv.org/abs/2406.05761.](https://arxiv.org/abs/2406.05761)

Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham
Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language
model specialized in evaluating other language models. _arXiv preprint arXiv:2405.01535_, 2024b.

Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward
models for language modeling. _arXiv preprint arXiv:2403.13787_, 2024.

Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge
for evaluating alignment. _arXiv preprint arXiv:2310.05470_, 2023a.

Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and
benchbuilder pipeline. _arXiv preprint arXiv:2406.11939_, 2024.

12

Published as a conference paper at ICLR 2025

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following
models, 2023b.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. _arXiv preprint_
_arXiv:2305.20050_, 2023.

Chris Yuhao Liu and Liang Zeng. Skywork reward model series. [https://huggingface.co/](https://huggingface.co/Skywork)
[Skywork, September 2024.](https://huggingface.co/Skywork) [URL https://huggingface.co/Skywork.](https://huggingface.co/Skywork)

Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, and Weizhu Chen. Arena learning: Build data flywheel for llms post-training via
simulated chatbot arena. _arXiv preprint arXiv:2407.10627_, 2024a.

Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun
Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated
process supervision. _arXiv preprint arXiv:2406.06592_, 2024b.

Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo,
Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. _arXiv preprint arXiv:2308.07124_, 2023.

OpenAI. How reasoning works, 2024. URL [https://platform.openai.com/docs/](https://platform.openai.com/docs/guides/reasoning/how-reasoning-works)
[guides/reasoning/how-reasoning-works.](https://platform.openai.com/docs/guides/reasoning/how-reasoning-works) Accessed: 2024-09-29.

OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red
Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher
Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann,
Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis,
Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila
Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Sim´on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan
Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan
Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu,
Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook
Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel
Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel
Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez,
Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv
Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney,
Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick,
Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David M´ely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe,
Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel
Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe
de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny,
Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders,

13

Published as a conference paper at ICLR 2025

Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor,
Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,
Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang,
Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer´on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan
Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,
Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming
Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL
[https://arxiv.org/abs/2303.08774.](https://arxiv.org/abs/2303.08774)

Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators, 2024.

Paul R¨ottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk
Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models.
_arXiv preprint arXiv:2308.01263_, 2023.

William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan
Leike. Self-critiquing models for assisting human evaluators. _arXiv preprint arXiv:2206.05802_,
2022\.

Tu Shiwen, Zhao Liang, Chris Yuhao Liu, Liang Zeng, and Yang Liu. Skywork critic model
series. [https://huggingface.co/Skywork,](https://huggingface.co/Skywork) September 2024. URL [https://](https://huggingface.co/Skywork)
[huggingface.co/Skywork.](https://huggingface.co/Skywork)

Skywork. Skywork-reward-preference-80k-v0.1. [https://huggingface.co/datasets/](https://huggingface.co/datasets/Skywork/Skywork-Reward-Preference-80K-v0.1)
[Skywork/Skywork-Reward-Preference-80K-v0.1, 2023.](https://huggingface.co/datasets/Skywork/Skywork-Reward-Preference-80K-v0.1) Accessed: 2024-09-18.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances_
_in Neural Information Processing Systems_, 33:3008–3021, 2020.

Mirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks
and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.

Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak, Laurent Sifre, Morgane Rivi\`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open
models based on gemini research and technology. _arXiv preprint arXiv:2403.08295_, 2024.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee
Lacroix, Baptiste Rozi\`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-based feedback. _arXiv preprint arXiv:2211.14275_, 2022.

Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady
Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries:
Evaluating llm generations with a panel of diverse models. _arXiv_ _preprint_ _arXiv:2404.18796_,
2024\.

Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu,
Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. _arXiv_ _preprint_
_arXiv:2305.17926_, 2023a.

14

Published as a conference paper at ICLR 2025

Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang
Sui. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. _arXiv_
_preprint arXiv:2312.08935_, 2023b.

Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,
Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm
instruction tuning optimization. _arXiv preprint arXiv:2306.05087_, 2023c.

Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming
Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging
multi-task language understanding benchmark. _arXiv preprint arXiv:2406.01574_, 2024a.

Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: A
dataset for evaluating safeguards in llms. _arXiv preprint arXiv:2308.13387_, 2023d.

Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang,
Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training
top-performing reward models, 2024b. [URL https://arxiv.org/abs/2406.08673.](https://arxiv.org/abs/2406.08673)

Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, and Mei Han. Systematic evaluation of
llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates, 2024.
[URL https://arxiv.org/abs/2408.13006.](https://arxiv.org/abs/2408.13006)

Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: A challenging, contaminationfree llm benchmark. _arXiv preprint arXiv:2406.19314_, 2024.

Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states
enables learning generalizable reward model for llms. _arXiv preprint arXiv:2406.10216_, 2024.

Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning in
mathematical reasoning. In _Findings of the Association for Computational Linguistics:_ _NAACL_
_2024_, pp. 858–875, 2024.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason
Weston. Self-rewarding language models. _arXiv preprint arXiv:2401.10020_, 2024.

Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large
language models at evaluating instruction following. _arXiv preprint arXiv:2310.07641_, 2023.

Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh
Agarwal. Generative verifiers: Reward modeling as next-token prediction. _arXiv_ _preprint_
_arXiv:2408.15240_, 2024.

Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu,
and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. _arXiv_ _preprint_
_arXiv:2308.01862_, 2023.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024.

Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm
helpfulness & harmlessness with rlaif, November 2023a.

Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models
are scalable judges. _arXiv preprint arXiv:2310.17631_, 2023b.

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv_
_preprint arXiv:1909.08593_, 2019.

15

Published as a conference paper at ICLR 2025

A APPENDIX

A.1 DETAILS OF THE JUDGES

We closely followed the official implementation of each judge, only making modifications where
necessary. One broad change we made across all judges is the use of greedy decoding (temperature=0) to ensure reproducibility. Any additional judge-specific modifications are detailed below.
We sourced proprietary LLMs through their official APIs. All open-weight LLMs (including reward
models) were served locally in half-precision, except for Llama-3.1-405B-Instruct for which we
utilized the TogetherAPI. The prompts for each judge are provided for reference in Appendix A.6.

A.1.1 PROMPTED JUDGES

**Vanilla** (Dubois et al., 2024): This is a basic judge prompted to output a label indicating which
response it believes to be better, with no explanation required. This judge has no tie option. Each
judgment must be no more than 1024 tokens, however, in practice, they contained significantly fewer
tokens.

**Arena-Hard Judge** (Li et al., 2024): This judge is used in LMSYS’s Arena-Hard Leaderboard [1] .
It’s prompted to provide its own response to the question to use as a reference before evaluating the
pair of candidate responses. This judge must decide between 5 options: A>>B, A>B, A=B, B>A,
or B>>A. We did not distinguish between the first two cases, nor did we distinguish between the last
two cases. Following the original implementation, each judgment must be less than 4096 tokens;
however, if we were unable to extract the verdict using regex (e.g., if the judgment was incomplete
after 4096 tokens), the judge was given one more opportunity to continue its judgment (up to 4096
additional tokens) and output a valid verdict. One special case worthy of note is the Arena-Hard
results with o1-mini and o1-preview may not have respected these token constraints nor the zero
temperature.

**Google** **Vertex** **Judge** (Cloud, 2024): Google Cloud offers a generative AI evaluation service
in Vertex AI powered by Gemini-Pro-001. It supports both single and pairwise evaluation, though
we only evaluated in the pairwise configuration using their predefined question-answering quality
metric. The service is proprietary and offers little to no ability to set generation parameters (e.g.,
temperature).

A.1.2 FINE-TUNED JUDGES

**PandaLM** (Wang et al., 2023c): PandaLM is a family of LLM-based judges based on LLaMA-7B
and LLaMA-70B (Touvron et al., 2023a), and fine-tuned on crowdsourced human preference data
collected by the authors. As of the time of publication, only the 7B variant of PandaLM has been
made publicly accessible, so we do not include results on the 70B variant. PandaLM supports a tie
option. We followed PandaLM’s official implementation closely, including beam searching over 4
beams. We made one crucial change to PandaLM’s official inference pipeline: we truncated both
candidate responses (from the left) to fit the request in the limited context window of 2048 tokens.
Left truncation was used as many of the responses in JudgeBench output their final decision at the
end; experimentally we found that left truncation performs better than right truncation. Although
PandaLM generates its decision before its explanation, we generated up to 150 tokens to give the
beams time to “mature.”

**Prometheus** **2** (Kim et al., 2024b): Prometheus 2 is a family judges fine-tuned from Mistral
7B (Jiang et al., 2023) and Mixtral 8x7B (Jiang et al., 2024). These models are fine-tuned separately
on both Feedback Collections (Kim et al., 2023), a direct-assessment synthetic dataset generated
by GPT-4 (OpenAI et al., 2024), and Preference Collections (Kim et al., 2024b), an augmented
version of Feedback Collections for pairwise evaluation, with the resulting weights merged. Additionally, the authors subsequently released a second Mixtral variant further trained on the BiGGen
Bench (Kim et al., 2024a). We evaluated all 3 Prometheus 2 models on JudgeBench. Prometheus 2

[1https://huggingface.co/spaces/lmsys/arena-hard-browser](https://huggingface.co/spaces/lmsys/arena-hard-browser)

16

Published as a conference paper at ICLR 2025

judges support fine-grained evaluation criteria, and we used their official “factual validity” criteria
since it best aligns with the motivation of JudgeBench. Prometheus 2 does not support ties.

**JudgeLM** (Zhu et al., 2023b): JudgeLM is a family of judges fine-tuned from Vicuna (Chiang
et al., 2023) using a dataset collected from existing instruction-tuning datasets that have been augmented with candidate responses and GPT-4 (OpenAI et al., 2024) judgments. JudgeLM assigns
integer scores to each candidate response, meaning ties are possible albeit unlikely. We made one
crucial change to JudgeLM’s official inference pipeline: we truncated both candidate responses
(from the left) to fit the request in the limited context window of 2048 tokens. Left truncation was
used as many of the responses in JudgeBench output their final decision at the end; experimentally
we found that left truncation performs better than right truncation. Since JudgeLM generates scores
before an explanation, we limited the number of generated tokens to just 16 to reduce the amount
that each candidate response is truncated.

**Auto-J** (Li et al., 2023a): Auto-J is a generative judge fine-tuned from Llama-2-13b-chat (Touvron et al., 2023b) on publicly available preference datasets augmented with GPT-4 (OpenAI et al.,
2024\) judgments. Auto-J supports ties. Following Auto-J’s official implementation, we generated
judgments up to 1024 tokens in length at a temperature of 0.

**Skywork** **Critics** (Shiwen et al., 2024): Skywork released a series of 8B and 70B generative
judges built on Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct (Dubey et al., 2024) respectively.
These models are fine-tuned on a combination of proprietary and open-source critic datasets. Following their official implementation, we generated judgments up to 2048 tokens at a temperature of
0\.

A.1.3 MULTI-AGENT JUDGES

**ChatEval** (Chan et al., 2023): ChatEval is a multi-agent judge that assigns roles to each agent.
Our implementation used two agents (one to act at the general public, while the other acts as a critic)
powered by GPT-4o as the model. The agents discuss sequentially (in a round-robin fashion) for at
most 4 turns. After the discussion, each agent independently assigns a score (between 1 and 10) for
each candidate response. We averaged the scores across both agents to determine the final decision.

A.2 ADDITIONAL ANALYSIS OF FINE-TUNED JUDGES

Many of the fine-tuned judges we evaluate score below the random guessing baseline of 50%. In
this section, we highlight three reasons this is the case: (1) truncated responses, (2) ties and invalid
decisions, and (3) inconsistent judgments. Moreover, we compare a few fine-tuned judges with pureprompting judges using the same base model to understand the impact of fine-tuning for judging.

**Truncated responses** Two of our judges (PandaLM and JudgeLM) have limited context windows
supporting just 2048 tokens. Combined, our candidate responses, however, often exceeded this limit.
As such, we dynamically truncated (from the left) both candidate responses (leaving all other parts
of the prompt template unchanged) to ensure the requests fit within the context limit. First, since
many candidate responses do not output their final answer until the end of their responses, truncating
from the left ensures the final answers are included in the truncated response provided to the judge.
Second, early experimentation with PandaLM and JudgeLM revealed that truncating from the left
resulted in better performance than truncating from the right.

**Ties** **and** **invalid** **decisions** Closely inspecting the generations of our fine-tuned judges reveals
several weaknesses. For example, out of 700 judgments (2 games across 350 examples), PandaLM
selected the tie option 479 times. Another Judge, Prometheus2-bgb-8x7b, made an invalid judgment
from which we cannot extract a decision on 215 of 700 judgments. Some of these invalid judgments
included “10/10”, “Neither A nor B”, and “3”. Table 5 provides the number of occurrences (out
of 700 games) each of the fine-tuned judges selects _A_ _>_ _B_, _A_ _\<_ _B_, _A_ = _B_, or the judgment is
otherwise invalid.

17

Published as a conference paper at ICLR 2025

Judge _A > B_ _A < B_ _A_ = _B_ Invalid

PandaLM-7B 45 114 479 62
Prometheus2-7b 395 232 0 73
Prometheus2-8x7b 331 328 0 41
Prometheus2-bgb-8x7b 239 215 0 246
JudgeLM-7B 399 229 72 0
JudgeLM-13B 355 312 33 0
JudgeLM-33B 344 264 92 0
AutoJ 289 378 33 0
Skywork-LLaMA-3.1B-8B 346 354 0 0
Skywork-LLaMA-3.1B-70B 390 310 0 0

Table 5: The prevalence of judgment types for each fine-tuned judge.

Judge Inconsistent

PandaLM-7B 29.14%
Prometheus2-7b 52.29%
Prometheus2-8x7b 40.00%
Prometheus2-bgb-8x7b 43.71%
JudgeLM-7B 59.71%
JudgeLM-13B 54.57%
JudgeLM-33B 38.00%
AutoJ 43.71%
Skywork-Llama-3.1B-8B 18.86%
Skywork-Llama-3.1B-70B 18.29%

Table 6: Rate of inconsistency between trials for each fine-tuned judge.

**Inconsistent judgments** Many fine-tuned judges struggled to generate consistent results between
games. For example, JudgeLM-7B and JudgeLM-13B were inconsistent on 59.71% and 54.57% of
pairs respectively. Likewise, Prometheus2-7b was inconsistent on 52.29% of pairs. Table 6 shares
the rate of inconsistency in judgments between games.

**Case** **Study:** **Do** **fine-tuned** **judges** **outperform** **prompted** **judges?** The fine-tuned SkyworkLlama-3.1B-8B and Skywork-LLaMA-3.1B-70B outperformed the corresponding arena-hard
judges with the same base model (Llama-3.1B-8B-Instruct and Llama-3.1B-70B-Instruct) by 12.57
and 5.14 respectively. Does this hold for other judges? In an attempt to answer this question, we
evaluated Mistral-7B-v0.1-Instruct, the base model behind Prometheus2-7b, using the Vanilla and
Arena-Hard prompts and present the results in Table 7. We found that Prometheus2-7b significantly
outperformed its base model (Mistral-7B-v0.1-Instruct) with both the vanilla prompt and ArenaHard prompt. It’s worth noting that with the vanilla prompt, Mistral-7B-v0.1-Instruct selected response A in 624/700 games, and with the arena-hard prompt, Mistral-7B-v0.1-Instruct selected the
tie option in 618/700 games. As such, it does appear that fine-tuned judges tend to outperform
prompted judges when using the same base model.

Judge Score

Prometheus2-7b 38.31
Vanilla (Mistral-7B-v0.1-Instruct) 7.43
Arena-Hard (Mistral-7B-v0.1-Instruct) 6.57

Table 7: Comparison of Prometheus2-7b against prompted judges with the same base model.

18

Published as a conference paper at ICLR 2025

A.3 DATASET FILTERING

We performed some additional filtering on MMLU-Pro and LiveBench. For MMLU-Pro, we randomly selected 100 questions from each of the 14 disciplines before generating responses and constructing pairs. Of these 1400 questions, only 347 and 233 contained both correct and incorrect
responses generated by GPT-4o and Claude-3.5-Sonnet, respectively. In both cases, we randomly
selected 11 pairs from each discipline, for a total of 154 knowledge pairs. Similarly, we randomly
sampled 100 questions from the math and reasoning subsets of Livebench. Note that we exclude the
“olympiad” subset of LiveBench-Math. We derived 98 reasoning pairs and 46 math pairs from GPT4o responses, but just 51 reasoning pairs and 34 math pairs from Claude-3.5-Sonnet responses. We
did no pre-filtering or post-filtering for LiveCodeBench, deriving 42 pairs from GPT-4o responses
and 31 pairs from Claude-3.5-Sonnet responses.

In all, the GPT-4o split of JudgeBench contains 350 instances, which is on par with similar benchmarks. For instance, FairEval (Wang et al., 2023a) contains 80 unique questions, LLMEval-2 (Zhang
et al., 2023) contains 480, MT-Bench (Zheng et al., 2024) contains 80, and LLMBar (Zeng et al.,
2023\) contains 419. RewardBench (Lambert et al., 2024) is larger, but it’s an aggregation of existing benchmarks, including MT-Bench and LLMBar. In order to test if this size is sufficient,
we augmented our “knowledge” subset, increasing the number of response pairs from 154 to 770.
We evaluated several LLMs using the Arena-Hard prompt, and observed that the relative rankings
among these judges were the same between our original set and the augmented set, despite small
variations in the scores themselves (see Table 8).

**Model** **Original Set** **Augmented Set**

gpt-4o 50.65 _(3rd)_ 46.49 _(3rd)_
gpt-4o-mini 48.05 _(4th)_ 44.03 _(4th)_
claude-3.5-sonnet 62.34 _(1st)_ 63.25 _(1st)_
claude-3-haiku 35.06 _(6th)_ 39.35 _(6th)_
llama-3.1-70b-instruct 51.30 _(2nd)_ 52.60 _(2nd)_
llama-3.1-8b-instruct 38.31 _(5th)_ 40.00 _(5th)_

Table 8: Performance on original and augmented “knowledge” sets.

A.4 ANALYSIS OF LENGTH BIAS ON JUDGE BENCH

Prior works has documented that LLM-based judges exhibit length bias, tending to prefer longer
responses over shorter ones (Hu et al., 2024; Wei et al., 2024). Because each of our pairs contains
two responses sampled from the same model, rather than from two different models, the responses
tend to be of similar length. On average, across all 350 instances of JudgeBench, correct and incorrect responses contain 562.29 and 561.16 tokens, respectively, using the GPT-4o tokenizer. This
negligible difference demonstrates that the construction of JudgeBench effectively mitigates length
bias, allowing LLM-based judges to be evaluated without this confounding factor.

A.5 EVALUATING RESPONSES FOR CORRECTNESS

For LiveBench and MMLU-Pro, we checked the correctness of generated responses using two methods. First, we parsed the final answer from the responses using regex and checked against the ground
truth answers. For LiveBench, we closely followed their official post-processing methodology to
extract the final answers. For MMLU-Pro, we adapted the multiple-choice questions to the same
format used by LiveBench and included in the question instructions to output its final letter choice 5
times in a row (e.g., “My final answer is AAAAA”). Using regex, we extracted the last capital letter
A-J that was repeated 5 times and treated this as the final answer. Second, we queried GPT-4o-mini
with the question, response, and ground truth (using the prompt provided below) and had it decide
whether or not the response was correct. We excluded any responses where these two methods
disagree.

19

Published as a conference paper at ICLR 2025

The advantage of this combined approach is to eliminate any questionable responses, such as in the
case where the model selects the correct answer but does not properly format its answer in accordance with the instructions. In general, we found high rates of agreement between these methods
(e.g., 97.7% and 99.5% agreement across 7000 responses to the 14000 MMLU-Pro questions for
GPT-4o and Claude-3.5-Sonnet, respectively). Below, we highlight an example case of disagreement, where the model correctly identified the correct answer, but formatted it incorrectly as IIIIi
instead of IIIII, which resulted in no regex match but GPT-4o-mini gave it the benefit of the doubt
and marked it correct.

20

Published as a conference paper at ICLR 2025

A.6 PROMPTS

Below, we share the prompts used for each judge we evaluated. These prompts come directly from
each judges’ official implementation, but are provided for reference.

21

Published as a conference paper at ICLR 2025

22

Published as a conference paper at ICLR 2025

23

Published as a conference paper at ICLR 2025

24

Published as a conference paper at ICLR 2025

25

Published as a conference paper at ICLR 2025

26

Published as a conference paper at ICLR 2025

27
