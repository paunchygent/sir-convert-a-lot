{
  "acceleration_used": "cpu",
  "backend_profile": "pymupdf",
  "backend_used": "pymupdf",
  "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
  "error_code": null,
  "job_id": "job_b14e9970c6b645d0a135a4968f",
  "latency_seconds": 6.379064,
  "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.md",
  "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.meta.json",
  "retry_warnings_count": 0,
  "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf",
  "source_size_bytes": 1079243,
  "status": "succeeded",
  "warnings": []
}
