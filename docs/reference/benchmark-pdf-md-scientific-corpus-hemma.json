{
  "acceptance_lane": {
    "gate_passed": true,
    "lane": "acceptance",
    "profiles": [
      {
        "job_spec_profile": {
          "acceleration_policy": "gpu_required",
          "backend_strategy": "auto",
          "normalize": "standard",
          "ocr_mode": "auto",
          "table_mode": "accurate"
        },
        "jobs": [
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "2025-emnlp-main-993-2ffebdb6",
            "error_code": null,
            "job_id": "job_48f4285ac15240d08a54b739b9",
            "latency_seconds": 27.364956,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/2025-emnlp-main-993-2ffebdb6.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/2025-emnlp-main-993-2ffebdb6.meta.json",
            "retry_warnings_count": 0,
            "source_file": "2025.emnlp-main.993.pdf",
            "source_size_bytes": 1316176,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "are-large-language-models-reliable-argument-quality-annotato-fcbe2290",
            "error_code": null,
            "job_id": "job_60f99d989b994c48aecef48c03",
            "latency_seconds": 9.252211,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/are-large-language-models-reliable-argument-quality-annotato-fcbe2290.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/are-large-language-models-reliable-argument-quality-annotato-fcbe2290.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Are Large Language Models Reliable Argument  Quality Annotators?.pdf",
            "source_size_bytes": 1423882,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "best-practices-for-text-annotation-with-large-language-model-09c36567",
            "error_code": null,
            "job_id": "job_29d5f83c1d8e486da059677f50",
            "latency_seconds": 4.588297,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/best-practices-for-text-annotation-with-large-language-model-09c36567.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/best-practices-for-text-annotation-with-large-language-model-09c36567.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Best Practices for Text Annotation with Large Language Models.pdf",
            "source_size_bytes": 357435,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "can-large-language-models-capture-human-annotator-disagreeme-cf75b84f",
            "error_code": null,
            "job_id": "job_bed74d19400f46fc8c564411c0",
            "latency_seconds": 23.159087,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/can-large-language-models-capture-human-annotator-disagreeme-cf75b84f.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/can-large-language-models-capture-human-annotator-disagreeme-cf75b84f.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Can Large Language Models Capture Human Annotator Disagreements?.pdf",
            "source_size_bytes": 611707,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6",
            "error_code": null,
            "job_id": "job_a3fa1fbd6d4f46d2b74d876cf0",
            "latency_seconds": 14.768458,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Dependence-Aware Label Aggregation for LLM-as-a-Judge  via Ising Models.pdf",
            "source_size_bytes": 722267,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
            "error_code": null,
            "job_id": "job_cf5961c266434d9885d390a825",
            "latency_seconds": 15.996915,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.meta.json",
            "retry_warnings_count": 0,
            "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf",
            "source_size_bytes": 1079243,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a",
            "error_code": null,
            "job_id": "job_b766b16edbb9430584f6ead98c",
            "latency_seconds": 20.231146,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Language Models in the Loop - Incorporating Prompting into Weak Supervision.pdf",
            "source_size_bytes": 2634156,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "meganno-a-human-llm-collaborative-annotation-system-530edb8d",
            "error_code": null,
            "job_id": "job_8e97ba7114614198a95573fca0",
            "latency_seconds": 4.176517,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/meganno-a-human-llm-collaborative-annotation-system-530edb8d.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/meganno-a-human-llm-collaborative-annotation-system-530edb8d.meta.json",
            "retry_warnings_count": 0,
            "source_file": "MEGAnno+- A Human-LLM Collaborative Annotation System.pdf",
            "source_size_bytes": 831814,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312",
            "error_code": null,
            "job_id": "job_9859209c46eb437faed2e3ebf7",
            "latency_seconds": 14.162607,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Prompt Stability Scoring for Text Annotation  with Large Language Models.pdf",
            "source_size_bytes": 6043317,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "auto",
            "backend_used": "docling",
            "document_slug": "the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d",
            "error_code": null,
            "job_id": "job_d93550a91ec2410485358d6760",
            "latency_seconds": 28.818658,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/acceptance/the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d.meta.json",
            "retry_warnings_count": 0,
            "source_file": "The Alternative Annotator Test for LLM-as-a-Judge-  How to Statistically Justify Replacing Human Annotators with LLMs .pdf",
            "source_size_bytes": 1019668,
            "status": "succeeded",
            "warnings": []
          }
        ],
        "profile_name": "auto",
        "summary": {
          "acceleration_usage": {
            "cuda": 10
          },
          "backend_usage": {
            "docling": 10
          },
          "canceled_jobs": 0,
          "failed_jobs": 0,
          "latency_seconds": {
            "max": 28.818658,
            "mean": 16.251885,
            "min": 4.176517,
            "p50": 15.382687,
            "p90": 27.510326,
            "p99": 28.687825
          },
          "retry_warnings_total": 0,
          "running_jobs": 0,
          "succeeded_jobs": 10,
          "success_rate": 1.0,
          "throughput_jobs_per_minute": 3.69137,
          "total_jobs": 10,
          "warnings_total": 0
        }
      }
    ],
    "service_url": "http://127.0.0.1:28085",
    "summary": {
      "acceleration_usage": {
        "cuda": 10
      },
      "backend_usage": {
        "docling": 10
      },
      "canceled_jobs": 0,
      "failed_jobs": 0,
      "latency_seconds": {
        "max": 28.818658,
        "mean": 16.251885,
        "min": 4.176517,
        "p50": 15.382687,
        "p90": 27.510326,
        "p99": 28.687825
      },
      "retry_warnings_total": 0,
      "running_jobs": 0,
      "succeeded_jobs": 10,
      "success_rate": 1.0,
      "throughput_jobs_per_minute": 3.690525,
      "total_jobs": 10,
      "warnings_total": 0
    }
  },
  "artifacts_root": "/Users/olofs_mba/Documents/Repos/sir-convert-a-lot/docs/reference/artifacts/task-12-scientific-corpus",
  "benchmark_id": "task-12-scientific-corpus",
  "corpus": {
    "count": 10,
    "files": [
      {
        "document_slug": "2025-emnlp-main-993-2ffebdb6",
        "source_file": "2025.emnlp-main.993.pdf",
        "source_size_bytes": 1316176
      },
      {
        "document_slug": "are-large-language-models-reliable-argument-quality-annotato-fcbe2290",
        "source_file": "Are Large Language Models Reliable Argument  Quality Annotators?.pdf",
        "source_size_bytes": 1423882
      },
      {
        "document_slug": "best-practices-for-text-annotation-with-large-language-model-09c36567",
        "source_file": "Best Practices for Text Annotation with Large Language Models.pdf",
        "source_size_bytes": 357435
      },
      {
        "document_slug": "can-large-language-models-capture-human-annotator-disagreeme-cf75b84f",
        "source_file": "Can Large Language Models Capture Human Annotator Disagreements?.pdf",
        "source_size_bytes": 611707
      },
      {
        "document_slug": "dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6",
        "source_file": "Dependence-Aware Label Aggregation for LLM-as-a-Judge  via Ising Models.pdf",
        "source_size_bytes": 722267
      },
      {
        "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
        "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf",
        "source_size_bytes": 1079243
      },
      {
        "document_slug": "language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a",
        "source_file": "Language Models in the Loop - Incorporating Prompting into Weak Supervision.pdf",
        "source_size_bytes": 2634156
      },
      {
        "document_slug": "meganno-a-human-llm-collaborative-annotation-system-530edb8d",
        "source_file": "MEGAnno+- A Human-LLM Collaborative Annotation System.pdf",
        "source_size_bytes": 831814
      },
      {
        "document_slug": "prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312",
        "source_file": "Prompt Stability Scoring for Text Annotation  with Large Language Models.pdf",
        "source_size_bytes": 6043317
      },
      {
        "document_slug": "the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d",
        "source_file": "The Alternative Annotator Test for LLM-as-a-Judge-  How to Statistically Justify Replacing Human Annotators with LLMs .pdf",
        "source_size_bytes": 1019668
      }
    ],
    "path": "/Users/olofs_mba/Documents/Repos/huledu-reboot/docs/research/research_papers/llm_as_a_annotater"
  },
  "decision": {
    "algorithm": "quality_first_v1",
    "follow_up_note": "Quality winner requires non-default governance profile; keep production recommendation governance-compatible and track follow-up decision task/ADR.",
    "follow_up_required": true,
    "quality_winner": "pymupdf",
    "ranking": [
      {
        "backend": "pymupdf",
        "latency_p50": 8.766455,
        "median_weighted_score": 3.0,
        "severe_quality_failures": 0,
        "success_rate": 1.0
      },
      {
        "backend": "docling",
        "latency_p50": 11.430146,
        "median_weighted_score": 3.0,
        "severe_quality_failures": 0,
        "success_rate": 1.0
      }
    ],
    "recommended_production_backend": "docling"
  },
  "evaluation_lane": {
    "gate_passed": true,
    "lane": "evaluation",
    "profiles": [
      {
        "job_spec_profile": {
          "acceleration_policy": "gpu_required",
          "backend_strategy": "docling",
          "normalize": "standard",
          "ocr_mode": "auto",
          "table_mode": "accurate"
        },
        "jobs": [
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "2025-emnlp-main-993-2ffebdb6",
            "error_code": null,
            "job_id": "job_313ef79c89254e9eaf9148b26a",
            "latency_seconds": 10.917338,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/2025-emnlp-main-993-2ffebdb6.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/2025-emnlp-main-993-2ffebdb6.meta.json",
            "retry_warnings_count": 0,
            "source_file": "2025.emnlp-main.993.pdf",
            "source_size_bytes": 1316176,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "are-large-language-models-reliable-argument-quality-annotato-fcbe2290",
            "error_code": null,
            "job_id": "job_d6dec217294d405d96cbe2e7ec",
            "latency_seconds": 8.356115,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/are-large-language-models-reliable-argument-quality-annotato-fcbe2290.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/are-large-language-models-reliable-argument-quality-annotato-fcbe2290.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Are Large Language Models Reliable Argument  Quality Annotators?.pdf",
            "source_size_bytes": 1423882,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "best-practices-for-text-annotation-with-large-language-model-09c36567",
            "error_code": null,
            "job_id": "job_5b4c55e9263341159ec579bfd7",
            "latency_seconds": 4.734645,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/best-practices-for-text-annotation-with-large-language-model-09c36567.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/best-practices-for-text-annotation-with-large-language-model-09c36567.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Best Practices for Text Annotation with Large Language Models.pdf",
            "source_size_bytes": 357435,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "can-large-language-models-capture-human-annotator-disagreeme-cf75b84f",
            "error_code": null,
            "job_id": "job_3619d486e7044951901e485399",
            "latency_seconds": 21.300569,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/can-large-language-models-capture-human-annotator-disagreeme-cf75b84f.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/can-large-language-models-capture-human-annotator-disagreeme-cf75b84f.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Can Large Language Models Capture Human Annotator Disagreements?.pdf",
            "source_size_bytes": 611707,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6",
            "error_code": null,
            "job_id": "job_0923d5c8da1e48129b58fa1c00",
            "latency_seconds": 13.396177,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Dependence-Aware Label Aggregation for LLM-as-a-Judge  via Ising Models.pdf",
            "source_size_bytes": 722267,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
            "error_code": null,
            "job_id": "job_5505ceab89ea4b2f9d10777520",
            "latency_seconds": 11.387951,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.meta.json",
            "retry_warnings_count": 0,
            "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf",
            "source_size_bytes": 1079243,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a",
            "error_code": null,
            "job_id": "job_046d948686704baeb106b98ace",
            "latency_seconds": 19.077014,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Language Models in the Loop - Incorporating Prompting into Weak Supervision.pdf",
            "source_size_bytes": 2634156,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "meganno-a-human-llm-collaborative-annotation-system-530edb8d",
            "error_code": null,
            "job_id": "job_34b67177ded34d97ad6d251644",
            "latency_seconds": 3.886231,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/meganno-a-human-llm-collaborative-annotation-system-530edb8d.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/meganno-a-human-llm-collaborative-annotation-system-530edb8d.meta.json",
            "retry_warnings_count": 0,
            "source_file": "MEGAnno+- A Human-LLM Collaborative Annotation System.pdf",
            "source_size_bytes": 831814,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312",
            "error_code": null,
            "job_id": "job_e57c45845ce54d76a5df0111cc",
            "latency_seconds": 11.472342,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Prompt Stability Scoring for Text Annotation  with Large Language Models.pdf",
            "source_size_bytes": 6043317,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cuda",
            "backend_profile": "docling",
            "backend_used": "docling",
            "document_slug": "the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d",
            "error_code": null,
            "job_id": "job_4f001820d5a649fbb05bd90231",
            "latency_seconds": 27.025668,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/docling/the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d.meta.json",
            "retry_warnings_count": 0,
            "source_file": "The Alternative Annotator Test for LLM-as-a-Judge-  How to Statistically Justify Replacing Human Annotators with LLMs .pdf",
            "source_size_bytes": 1019668,
            "status": "succeeded",
            "warnings": []
          }
        ],
        "profile_name": "docling",
        "summary": {
          "acceleration_usage": {
            "cuda": 10
          },
          "backend_usage": {
            "docling": 10
          },
          "canceled_jobs": 0,
          "failed_jobs": 0,
          "latency_seconds": {
            "max": 27.025668,
            "mean": 13.155405,
            "min": 3.886231,
            "p50": 11.430146,
            "p90": 21.873079,
            "p99": 26.510409
          },
          "retry_warnings_total": 0,
          "running_jobs": 0,
          "succeeded_jobs": 10,
          "success_rate": 1.0,
          "throughput_jobs_per_minute": 4.56006,
          "total_jobs": 10,
          "warnings_total": 0
        }
      },
      {
        "job_spec_profile": {
          "acceleration_policy": "cpu_only",
          "backend_strategy": "pymupdf",
          "normalize": "standard",
          "ocr_mode": "off",
          "table_mode": "accurate"
        },
        "jobs": [
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "2025-emnlp-main-993-2ffebdb6",
            "error_code": null,
            "job_id": "job_d15bce8438fd4df8a77a6a22b9",
            "latency_seconds": 9.351925,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/2025-emnlp-main-993-2ffebdb6.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/2025-emnlp-main-993-2ffebdb6.meta.json",
            "retry_warnings_count": 0,
            "source_file": "2025.emnlp-main.993.pdf",
            "source_size_bytes": 1316176,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "are-large-language-models-reliable-argument-quality-annotato-fcbe2290",
            "error_code": null,
            "job_id": "job_ba2590b10e834a9d85a0ccc4c8",
            "latency_seconds": 4.108924,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/are-large-language-models-reliable-argument-quality-annotato-fcbe2290.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/are-large-language-models-reliable-argument-quality-annotato-fcbe2290.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Are Large Language Models Reliable Argument  Quality Annotators?.pdf",
            "source_size_bytes": 1423882,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "best-practices-for-text-annotation-with-large-language-model-09c36567",
            "error_code": null,
            "job_id": "job_3be99160b1d248f592ebf617a8",
            "latency_seconds": 2.798653,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/best-practices-for-text-annotation-with-large-language-model-09c36567.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/best-practices-for-text-annotation-with-large-language-model-09c36567.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Best Practices for Text Annotation with Large Language Models.pdf",
            "source_size_bytes": 357435,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "can-large-language-models-capture-human-annotator-disagreeme-cf75b84f",
            "error_code": null,
            "job_id": "job_dfad6106635e418d93bd17af3c",
            "latency_seconds": 10.942945,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/can-large-language-models-capture-human-annotator-disagreeme-cf75b84f.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/can-large-language-models-capture-human-annotator-disagreeme-cf75b84f.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Can Large Language Models Capture Human Annotator Disagreements?.pdf",
            "source_size_bytes": 611707,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6",
            "error_code": null,
            "job_id": "job_53425fd55d4b411883a0e21c05",
            "latency_seconds": 10.221771,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Dependence-Aware Label Aggregation for LLM-as-a-Judge  via Ising Models.pdf",
            "source_size_bytes": 722267,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
            "error_code": null,
            "job_id": "job_b14e9970c6b645d0a135a4968f",
            "latency_seconds": 6.379064,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8.meta.json",
            "retry_warnings_count": 0,
            "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf",
            "source_size_bytes": 1079243,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a",
            "error_code": null,
            "job_id": "job_818a04481bd0468ba7a97ba8e6",
            "latency_seconds": 28.215141,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Language Models in the Loop - Incorporating Prompting into Weak Supervision.pdf",
            "source_size_bytes": 2634156,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "meganno-a-human-llm-collaborative-annotation-system-530edb8d",
            "error_code": null,
            "job_id": "job_46258e3a941841b3bb034f7012",
            "latency_seconds": 2.638564,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/meganno-a-human-llm-collaborative-annotation-system-530edb8d.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/meganno-a-human-llm-collaborative-annotation-system-530edb8d.meta.json",
            "retry_warnings_count": 0,
            "source_file": "MEGAnno+- A Human-LLM Collaborative Annotation System.pdf",
            "source_size_bytes": 831814,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312",
            "error_code": null,
            "job_id": "job_5e41ce9ce5eb46ee932bd8610a",
            "latency_seconds": 8.180986,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312.meta.json",
            "retry_warnings_count": 0,
            "source_file": "Prompt Stability Scoring for Text Annotation  with Large Language Models.pdf",
            "source_size_bytes": 6043317,
            "status": "succeeded",
            "warnings": []
          },
          {
            "acceleration_used": "cpu",
            "backend_profile": "pymupdf",
            "backend_used": "pymupdf",
            "document_slug": "the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d",
            "error_code": null,
            "job_id": "job_ba172ef0e7024a908f77bc68a0",
            "latency_seconds": 13.564009,
            "output_markdown_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d.md",
            "output_metadata_path": "docs/reference/artifacts/task-12-scientific-corpus/evaluation/pymupdf/the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d.meta.json",
            "retry_warnings_count": 0,
            "source_file": "The Alternative Annotator Test for LLM-as-a-Judge-  How to Statistically Justify Replacing Human Annotators with LLMs .pdf",
            "source_size_bytes": 1019668,
            "status": "succeeded",
            "warnings": []
          }
        ],
        "profile_name": "pymupdf",
        "summary": {
          "acceleration_usage": {
            "cpu": 10
          },
          "backend_usage": {
            "pymupdf": 10
          },
          "canceled_jobs": 0,
          "failed_jobs": 0,
          "latency_seconds": {
            "max": 28.215141,
            "mean": 9.640198,
            "min": 2.638564,
            "p50": 8.766455,
            "p90": 15.029122,
            "p99": 26.896539
          },
          "retry_warnings_total": 0,
          "running_jobs": 0,
          "succeeded_jobs": 10,
          "success_rate": 1.0,
          "throughput_jobs_per_minute": 6.2226,
          "total_jobs": 10,
          "warnings_total": 0
        }
      }
    ],
    "service_url": "http://127.0.0.1:28086",
    "summary": {
      "acceleration_usage": {
        "cpu": 10,
        "cuda": 10
      },
      "backend_usage": {
        "docling": 10,
        "pymupdf": 10
      },
      "canceled_jobs": 0,
      "failed_jobs": 0,
      "latency_seconds": {
        "max": 28.215141,
        "mean": 11.397802,
        "min": 2.638564,
        "p50": 10.569555,
        "p90": 21.873079,
        "p99": 27.989141
      },
      "retry_warnings_total": 0,
      "running_jobs": 0,
      "succeeded_jobs": 20,
      "success_rate": 1.0,
      "throughput_jobs_per_minute": 5.262943,
      "total_jobs": 20,
      "warnings_total": 0
    }
  },
  "generated_at": "2026-02-15T01:28:14Z",
  "governance_compatibility": {
    "notes": [
      "Quality winner requires non-default governance profile; keep production recommendation governance-compatible and track follow-up decision task/ADR."
    ],
    "production_profile": {
      "acceleration_policy": "gpu_required",
      "backend_strategy": "auto",
      "normalize": "standard",
      "ocr_mode": "auto",
      "table_mode": "accurate"
    },
    "quality_winner": "pymupdf",
    "quality_winner_compatible_for_production": false,
    "recommended_production_backend": "docling"
  },
  "quality_rubric": {
    "auto_generated": true,
    "entries": [
      {
        "backend": "docling",
        "document_slug": "2025-emnlp-main-993-2ffebdb6",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "2025.emnlp-main.993.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "are-large-language-models-reliable-argument-quality-annotato-fcbe2290",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Are Large Language Models Reliable Argument  Quality Annotators?.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "best-practices-for-text-annotation-with-large-language-model-09c36567",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Best Practices for Text Annotation with Large Language Models.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "can-large-language-models-capture-human-annotator-disagreeme-cf75b84f",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Can Large Language Models Capture Human Annotator Disagreements?.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Dependence-Aware Label Aggregation for LLM-as-a-Judge  via Ising Models.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Language Models in the Loop - Incorporating Prompting into Weak Supervision.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "meganno-a-human-llm-collaborative-annotation-system-530edb8d",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "MEGAnno+- A Human-LLM Collaborative Annotation System.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Prompt Stability Scoring for Text Annotation  with Large Language Models.pdf"
      },
      {
        "backend": "docling",
        "document_slug": "the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "The Alternative Annotator Test for LLM-as-a-Judge-  How to Statistically Justify Replacing Human Annotators with LLMs .pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "2025-emnlp-main-993-2ffebdb6",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "2025.emnlp-main.993.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "are-large-language-models-reliable-argument-quality-annotato-fcbe2290",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Are Large Language Models Reliable Argument  Quality Annotators?.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "best-practices-for-text-annotation-with-large-language-model-09c36567",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Best Practices for Text Annotation with Large Language Models.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "can-large-language-models-capture-human-annotator-disagreeme-cf75b84f",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Can Large Language Models Capture Human Annotator Disagreements?.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "dependence-aware-label-aggregation-for-llm-as-a-judge-via-is-bc244bc6",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Dependence-Aware Label Aggregation for LLM-as-a-Judge  via Ising Models.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "judgebench-a-benchmark-for-evaluating-llm-based-judges-9f8194b8",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "JUDGEBENCH- A BENCHMARK FOR EVALUATING  LLM-BASED JUDGES.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "language-models-in-the-loop-incorporating-prompting-into-wea-c3bb110a",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Language Models in the Loop - Incorporating Prompting into Weak Supervision.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "meganno-a-human-llm-collaborative-annotation-system-530edb8d",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "MEGAnno+- A Human-LLM Collaborative Annotation System.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "prompt-stability-scoring-for-text-annotation-with-large-lang-99b12312",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "Prompt Stability Scoring for Text Annotation  with Large Language Models.pdf"
      },
      {
        "backend": "pymupdf",
        "document_slug": "the-alternative-annotator-test-for-llm-as-a-judge-how-to-sta-d936662d",
        "information_retention": 3,
        "layout_fidelity": 3,
        "legibility": 3,
        "notes": "autogenerated placeholder rubric; replace with manual review notes.",
        "source_file": "The Alternative Annotator Test for LLM-as-a-Judge-  How to Statistically Justify Replacing Human Annotators with LLMs .pdf"
      }
    ],
    "generated_at": "2026-02-15T01:28:14Z",
    "weights": {
      "information_retention": 0.35,
      "layout_fidelity": 0.45,
      "legibility": 0.2
    }
  },
  "service_revision": {
    "hemma_sha": "459fb6f5744b2840839eb8fd6aace497d3e1d6f3",
    "local_sha": "459fb6f5744b2840839eb8fd6aace497d3e1d6f3"
  }
}
